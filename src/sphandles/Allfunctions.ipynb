{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(fname):\n",
    "    pattern = \".*__(\\w+).csv\"\n",
    "    m = re.match(pattern, fname)\n",
    "    return m.group(1)\n",
    "\n",
    "def read_csv(fname,dropcols=None, dropzero = None):\n",
    "    dropcols = dropcols if dropcols else []\n",
    "    label = parse_filename(fname)\n",
    "    df = pd.read_csv('./{}'.format(fname),\n",
    "                    index_col=0, engine = 'openpyxl')\n",
    "    df = df[[c for c in df.columns if c.endswith('mass_g') and c not in dropcols]].fillna(0.0)\n",
    "    df.columns = df.columns.str.replace('_mass_g','')\n",
    "    if dropzero != None:\n",
    "        df = df[abs(df).T.sum() > 0].reset_index(drop=True)\n",
    "        df[df < 0] = 0\n",
    "        df['label'] = label\n",
    "        df = df.set_index('label')\n",
    "    return df, label\n",
    "\n",
    "def pca_analysis(df, label):\n",
    "    pca = PCA()\n",
    "    X = pca.fit_transform(df)\n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x1, x2, '.')\n",
    "    ax.set(\n",
    "        xlabel=\"First component\",\n",
    "        ylabel=\"Second component\",\n",
    "        title=\"%s: projected onto first two components\" % label,\n",
    "    )\n",
    "    \n",
    "    components = pd.DataFrame(pca.components_.T, index=df.columns)\n",
    "    print_until = 0.9999\n",
    "    cumulative_explained_variance = 0\n",
    "    i = 0\n",
    "    while cumulative_explained_variance < print_until:\n",
    "        explained = pca.explained_variance_ratio_[i]\n",
    "        print(\"=> COMPONENT %d explains %0.2f%% of the variance\" % (i, 100*explained))\n",
    "        top_components = components[i].sort_values(ascending=False)\n",
    "        top_components = top_components[abs(top_components) > 0.0]\n",
    "        print(top_components)\n",
    "        i += 1\n",
    "        cumulative_explained_variance += explained\n",
    "    return components\n",
    "\n",
    "# TODO\n",
    "def nmf_analysis(df, label):\n",
    "    nmf = NMF(components=2)\n",
    "    X = nmf.fit_transform(df)\n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x1, x2, '.')\n",
    "    ax.set(\n",
    "        xlabel=\"First component\",\n",
    "        ylabel=\"Second component\",\n",
    "        title=\"%s: projected onto first two components\" % label,\n",
    "    )\n",
    "    error = nmf.reconstruction_err_ / float(len(df))\n",
    "    components = pd.DataFrame(nmf.components_.T, index=df.columns)\n",
    "\n",
    "def load_and_analyze(filenames, dropcols=None):\n",
    "    for filename in filenames:\n",
    "        df, label = read_csv(filename, dropcols)\n",
    "        print(\"== %s ==\" % label)\n",
    "        components = pca_analysis(df, label)\n",
    "        print()\n",
    "        \n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, classes)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "def evaluate_model(clf, df, labels):\n",
    "    model_name = '->'.join(next(zip(*clf.steps)))\n",
    "    print(\"Model: %s\" % model_name)\n",
    "    scores = cross_val_score(clf, df, labels, cv=3) #cv=TimeSeriesSplit(3))\n",
    "    mean_score = scores.mean()\n",
    "    std_score = scores.std()\n",
    "    print(\"Accuracy: [%0.4f, %0.4f] (%0.4f +/- %0.4f)\"\n",
    "          % (mean_score - std_score, mean_score + std_score, mean_score, std_score))\n",
    "    y_pred = cross_val_predict(clf, df, labels, cv=3)\n",
    "    y_prob = cross_val_predict(clf, df, labels, cv=3, method='predict_proba')\n",
    "    plot_confusion_matrix(labels, y_pred, labels.unique())\n",
    "    return y_pred, y_prob, labels, df\n",
    "\n",
    "def evaluate_modelrfe(clf, df, labels):\n",
    "    model_name = '->'.join(next(zip(*clf.steps)))\n",
    "    print(\"Model: %s\" % model_name)\n",
    "    scores = cross_val_score(clf, df, labels, cv=3)\n",
    "    mean_score = scores.mean()\n",
    "    std_score = scores.std()\n",
    "    print(\"Accuracy: [%0.4f, %0.4f] (%0.4f +/- %0.4f)\"\n",
    "          % (mean_score - std_score, mean_score + std_score, mean_score, std_score))\n",
    "    y_pred = cross_val_predict(clf, df, labels, cv=3)\n",
    "    y_prob = cross_val_predict(clf, df, labels, cv=3, method='predict')\n",
    "    return y_pred, y_prob, labels, df\n",
    "\n",
    "#Type: Data Frame. From a selected isotope and its particle events, select all other isotopes associated and drop others not associated with it\n",
    "def isotope_particle(data, isotope):\n",
    "    obs = data[data[isotope] > 0.0]\n",
    "    return obs\n",
    "\n",
    "#Merging particle splits based on analyte\n",
    "def merge_particles(df,dfc):\n",
    "    index_selector = []\n",
    "    for i in dfc.columns:\n",
    "        index_selector1 = dfc[dfc[i].notnull()][i].cumsum()\n",
    "        index_selector.append(index_selector1)\n",
    "    dfcorrected = df.groupby(index_selector[13]).agg('sum').reset_index(drop=True).drop(columns = 'ID')\n",
    "    dfcorrected = dfcorrected.replace(0, np.nan)\n",
    "    return dfcorrected\n",
    "\n",
    "#displays R-Sqr value for each dissolved calibration\n",
    "def r_sqr_analyzer(data, R_value):\n",
    "    c = data[data['r_sqr_counts'] < R_value]\n",
    "    return c\n",
    "\n",
    "\n",
    "#user function: loads a list of variable path names and assigns a label \n",
    "def load_and_label(pathname, newlabel):\n",
    "    data1 = pd.DataFrame()\n",
    "    for i in glob.glob(pathname):\n",
    "        data, soil_label = read_csv(i,DROPCOLS)\n",
    "        data1 = pd.concat([data,data1], axis = 0, sort=False)\n",
    "    data1['newlabel'] = newlabel\n",
    "    data1 = data1.set_index('newlabel')\n",
    "    return data1\n",
    "\n",
    "#bootstrapping method\n",
    "# parsing DF for test dataset\n",
    "def holdoutdata(df, perc):\n",
    "    #making hold out data\n",
    "    dfnat = df[df.index == 'Natural']\n",
    "    dfeng = df[df.index == 'Engineered']\n",
    "\n",
    "    msk = np.random.rand(len(dfnat)) < perc\n",
    "    mske = np.random.rand(len(dfeng)) < perc\n",
    "    holdoutdatanat = dfnat[~msk]\n",
    "    holdoutdataeng = dfeng[~mske]\n",
    "    trainingdatanat = dfnat[msk]\n",
    "    trainingdataeng = dfeng[mske]\n",
    "\n",
    "    #combining holdout data together and labels\n",
    "    holdoutdata = pd.concat([holdoutdatanat, holdoutdataeng], axis=0)\n",
    "    holdoutlabels = holdoutdata.index.get_level_values(level='newlabel')\n",
    "    return holdoutdata, holdoutlabels, trainingdatanat, trainingdataeng\n",
    "\n",
    "#bootstrapping process. Holdout some data, predict on it and do that N amount of times. Repeated this procedure M amount of times \n",
    "def bootstrap(df, N, m):\n",
    "    #repeat this process M amount of times\n",
    "    for _ in range(m):\n",
    "        #holdout 20% of data. \n",
    "        holdoutdata, holdoutlabels, trainingdatanat, trainingeng = holdoutdata(df, 0.2)\n",
    "        #create a NP array from 0.05 to 1.05 with 0.05 step increment\n",
    "        p = np.arange(0.05, 1.05, 0.05)\n",
    "        #create an empty dataframe\n",
    "        df1 = pd.DataFrame()\n",
    "        #for each 0.05 step from p\n",
    "        for i in p:\n",
    "            #create an empty list\n",
    "            percent = []\n",
    "            #repeat this N amount of times\n",
    "            for _ in range(N):\n",
    "                #sample an i fraction of the training data\n",
    "                snat = trainingdatanat.sample(frac = i, replace = True)\n",
    "                seng = trainingdataeng.sample(frac = i, replace = True)\n",
    "                \n",
    "                #combine training data and get traininglabels\n",
    "\n",
    "                trainingdata = pd.concat([snat, seng], axis=0)\n",
    "                traininglabels = trainingdata.index.get_level_values(level='newlabel')\n",
    "                \n",
    "                #create the logistic Regression model with 5-fold cross validation with NMF of 10 componenets\n",
    "            \n",
    "                clf = make_pipeline(\n",
    "                    StandardScaler(with_mean=False),\n",
    "                    NMF(n_components=10),\n",
    "                    LogisticRegressionCV(cv=5, max_iter=300)\n",
    "                )\n",
    "                #fit with fraction of training set\n",
    "                clf = clf.fit(trainingdata, traininglabels)\n",
    "                #score with the heldout test set\n",
    "                y_score = clf.score(holdoutdata, holdoutlabels)\n",
    "                #append the score to percent\n",
    "                percent.append(y_score)\n",
    "                #put it in a dataframe format\n",
    "                pcdf = pd.DataFrame(percent)\n",
    "            df1 = pd.concat([df1, pcdf], axis = 1)\n",
    "        \n",
    "        df1.columns = np.arange(0.05, 1.05, 0.05)\n",
    "        \n",
    "        average = []\n",
    "        stdev = []\n",
    "        for i in df1.columns:\n",
    "            average.append(df1[i].mean())\n",
    "            stdev.append(df1[i].std())\n",
    "        plt.errorbar(np.arange(0.05, 1.05, 0.05), average, yerr = stdev)\n",
    "        plt.xlabel('% of training data')\n",
    "        plt.ylabel('Accuracy of model (%)')\n",
    "\n",
    "#return df_with_prob\n",
    "def just_data(data):\n",
    "    new = data.drop(columns = ['newlabel','Engineered','Natural','prob_obs_group'])\n",
    "    return new\n",
    "\n",
    "#marginal_probabilities returns out of the total particle events, what is the probability of an individual isotope occurring\n",
    "def marginal_probabilities(data):\n",
    "    #return data.count().sort_values(ascending=False) / total_particles(data).sum()\n",
    "    return (abs(data) > 0.0).mean().sort_values(ascending=False)\n",
    "\n",
    "#Counts total number of peaks for each isotope\n",
    "def marginal_particle(data):\n",
    "    return data[data > 0.0].count().sort_values(ascending=False)\n",
    "\n",
    "def non_zero_data(data):\n",
    "    non_zero_rows = data.abs().sum(axis=1) > 0.0\n",
    "    non_zero_data = data[non_zero_rows]\n",
    "    non_zero_columns = non_zero_data.abs().sum(axis=0) > 0.0\n",
    "    non_zero_data = non_zero_data.loc[: , non_zero_columns]\n",
    "    return non_zero_data\n",
    "\n",
    "#counts the total particles associated to an isotope\n",
    "#conditional_probabilities returns out of the total selected-isotope particle events, what is the probability it is associated with the list of other isotopes \n",
    "def conditional_probabilities(data, isotope):\n",
    "    obs = data[abs(data[isotope]) > 0.0]\n",
    "    partners = (abs(obs) > 0.0).astype(np.float64).mean()\n",
    "    return partners[abs(partners) > 0.0].sort_values(ascending=False)\n",
    "\n",
    "#counts the total particles associated to an isotope\n",
    "def conditional_particle(data, isotope):\n",
    "    obs = data[abs(data[isotope]) > 0.0].count()\n",
    "    return obs.sort_values(ascending=False)\n",
    "\n",
    "#isotope_pure returns a data frame of the selected isotope particle event and its impurities\n",
    "def isotope_pure(data, isotope):\n",
    "    obs = data[data[isotope] > 0.0]\n",
    "    others = obs.drop(columns=isotope)\n",
    "    pure = others.sum(axis=1) == 0.0\n",
    "    others = obs[pure]\n",
    "    return others\n",
    "\n",
    "#probability_pure returns out of the total selected-isotope particle events, what is the probability that it is not associated with any other isotope\n",
    "def probability_pure(data, isotope):\n",
    "    obs = data[data[isotope] > 0.0]\n",
    "    others = obs.drop(columns=isotope)\n",
    "    pure = (others.sum(axis=1) == 0.0).mean()\n",
    "    return pure\n",
    "\n",
    "#returns the same number of particles to the smallest dataframe\n",
    "def sameamount(dfA, dfB):\n",
    "    if len(dfA) > len(dfB):\n",
    "        dfC = pd.concat([dfA[:len(dfB)], dfB], sort=False).fillna(0.0).clip(lower=0.0)\n",
    "    else:\n",
    "        dfC = pd.concat([dfA, dfB[:len(dfA)]], sort=False).fillna(0.0).clip(lower=0.0)\n",
    "\n",
    "    labelsA = dfC.index.get_level_values(level='newlabel')\n",
    "    return dfC, labelsA\n",
    "\n",
    "def category_split(labeldf, label, upperbound, lowerbound):\n",
    "        correct = labeldf[labeldf[label] > upperbound]\n",
    "        uncertain = labeldf[labeldf[label] < upperbound]\n",
    "        uncertain = uncertain[uncertain[label] > lowerbound]\n",
    "        misclassified = labeldf[labeldf[label] < lowerbound]\n",
    "        return [correct, uncertain, misclassified]\n",
    "    \n",
    "def graphmaker(df, label, upperbound, name, natdf, savefigs, opencircles = False):\n",
    "    toppercent = conditional_probabilities(just_data(df[df[label] > upperbound]), '48Ti')\n",
    "    bottompercent = conditional_probabilities(just_data(df[df[label] < upperbound]), '48Ti')\n",
    "    percentdf = pd.DataFrame([toppercent, bottompercent]).T\n",
    "    percentdf.columns = ['toppercent', 'bottompercent']\n",
    "    percentdf.reset_index(inplace = True)\n",
    "    percentdf = pd.melt(percentdf[1:13], id_vars = ['index'], value_vars = ['toppercent', 'bottompercent'])\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16,4), dpi=300, gridspec_kw={'width_ratios': [2.5, 1]})\n",
    "    ax1 = sns.barplot('index', 'value', 'variable', data = percentdf, ax = axs[0])\n",
    "    ax1.set_ylabel('Isotope Frequency', fontsize = 16)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax1.set_xlabel('')\n",
    "    ax1.set_ylim([10**-3, 1])\n",
    "    ax1.legend_.remove()\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    #if you want unassociated Ti as a separate category\n",
    "    if opencircles == True:\n",
    "        #make the df pure ti df\n",
    "        Pure = isotope_pure(just_data(df).drop(columns = '46Ti'), '48Ti')\n",
    "        percentdfpure = df.loc[Pure.index]\n",
    "        df.drop(percentdfpure.index, inplace = True)\n",
    "        \n",
    "    #parse particles by confidence probability to the correct category\n",
    "    top = df[df[label] > upperbound]\n",
    "    puretop = percentdfpure[percentdfpure[label] > upperbound]\n",
    "    bot = df[df[label] < upperbound]\n",
    "    purebot = percentdfpure[percentdfpure[label] < upperbound]\n",
    "\n",
    "    #ax2 = sns.scatterplot('48Ti', label, data = percentdfpure, ax = axs[1], edgecolors = 'red')    \n",
    "    axs[1].scatter(bot['48Ti'], bot[label], linewidths = 0.75, edgecolors = 'white', facecolors = '#ff7f0e')\n",
    "    axs[1].scatter(top['48Ti'], top[label], linewidths = 0.75, edgecolors = 'white', facecolors = '#1f77b4')\n",
    "    axs[1].scatter(purebot['48Ti'], purebot[label], linewidths = 0.75, edgecolors = 'white', facecolors = '#ff7f0e', marker = '^')\n",
    "    axs[1].scatter(puretop['48Ti'], puretop[label], linewidths = 0.75, edgecolors = 'white', facecolors = '#1f77b4', marker = '^')\n",
    "    axs[1].set_xscale('log')\n",
    "    axs[1].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    axs[1].set_xlim([min(natdf['48Ti']), 10**-13])\n",
    "    axs[1].set_xlabel('Ti Mass (g)')\n",
    "    axs[1].set_ylabel('Prediction Probability of ' + str(label) + ' Category')\n",
    "    axs[1].set_ylim(0.0)\n",
    "    if savefigs == None:\n",
    "        plt.savefig('../../figsfromscript/fingerprints' + str(label) + str(name) + '.png', dpi = 300)\n",
    "    \n",
    "def graphmakerm(df, label, upperbound, lowerbound, name, natdf, savefigs, opencircles = False):\n",
    "    toppercent = conditional_probabilities(just_data(df[df[label] > upperbound]), '48Ti')\n",
    "    middlepercent = df[df[label] < upperbound]\n",
    "    middlepercent = conditional_probabilities(just_data(middlepercent[middlepercent[label] > lowerbound]), '48Ti')\n",
    "    bottompercent = conditional_probabilities(just_data(df[df[label] < lowerbound]), '48Ti')\n",
    "    percentdf = pd.DataFrame([toppercent, middlepercent, bottompercent]).T\n",
    "    percentdf.columns = ['toppercent', 'middlepercent', 'bottompercent']\n",
    "    percentdf.reset_index(inplace = True)\n",
    "    percentdf = pd.melt(percentdf[1:13], id_vars = ['index'], value_vars = ['toppercent', 'middlepercent', 'bottompercent'])\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(18,4), dpi=300, gridspec_kw={'width_ratios': [3, 1]})\n",
    "    ax1 = sns.barplot('index', 'value', 'variable', data = percentdf, palette = 'Blues', ax = axs[0])\n",
    "    ax1.set_ylabel('Isotope Frequency')\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax1.set_xlabel('')\n",
    "    ax1.legend_.remove()\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    #if opencircles == True:\n",
    "        #Pure = isotope_pure(just_data(df).drop(columns = '46Ti'), label)\n",
    "\n",
    "    top = df[df[label] > upperbound]\n",
    "    top['category'] = 'Top ' + str(100-upperbound*100) + '%'\n",
    "    middle = df[df[label] < upperbound]\n",
    "    middle = middle[middle[label] > lowerbound]\n",
    "    middle['category'] = 'Middle' + str((upperbound - lowerbound)*100) + '%'\n",
    "    bot = df[df[label] < lowerbound]\n",
    "    bot['category'] = 'Bottom ' + str(upperbound*100) + '%'\n",
    "\n",
    "\n",
    "    df = pd.concat([bot, middle, top], axis = 0)\n",
    "\n",
    "    ax2 = sns.scatterplot('48Ti', label, 'category', data = df, palette = 'Blues_d', ax = axs[1])\n",
    "    ax2 = sns.scatterplot('48Ti', label, 'category', data = percentdfpure, palette = 'Blues_d', ax = axs[1], markers= '+')\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.set_xlim([min(natdf['48Ti']), 10**-13])\n",
    "    ax2.set_xlabel('Ti Mass (g)')\n",
    "    ax2.legend_.remove()\n",
    "    ax2.set_ylabel('Prediction Probability of ' + str(label) + ' Category')\n",
    "    if savefigs == None:\n",
    "        plt.savefig('../../figsfromscript/fingerprints' + str(label) + str(name) + '.png', dpi = 300)\n",
    "    \n",
    "def get_ti(x):\n",
    "    return x[:, [6]]\n",
    "\n",
    "\n",
    "#isotope_notisotope returns a data frame of the selected isotope particle event not related to another selected isotope\n",
    "def isotope_notisotope(data, isotope1, isotope2):\n",
    "    obs = data[data[isotope1] > 0.0]\n",
    "    obs = obs[obs[isotope2] == 0.0]\n",
    "    return obs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
